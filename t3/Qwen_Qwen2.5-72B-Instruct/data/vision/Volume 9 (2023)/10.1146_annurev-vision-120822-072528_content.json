{"Literature Review": "The ability to predict visual fixations has been a focal point in cognitive science and computer vision due to its implications for understanding human perception and behavior. Visual fixations, or the points where the eyes momentarily stop during saccadic movements, provide valuable insights into the cognitive processes underlying attention and perception. This literature review synthesizes recent advancements in predicting visual fixations, focusing on the evaluation and comparison of models, the role of probabilistic frameworks, and the contributions of various factors. Recent studies have highlighted the importance of probabilistic models in predicting visual fixations. These models offer a unified approach to fixation prediction, allowing researchers to compare different models across various settings, such as static images and videos. For instance, Kümmerer et al. (2016) introduced a deep learning-based model that uses a probabilistic framework to predict fixations on natural images. Their model, which incorporates both bottom-up and top-down attention mechanisms, outperformed traditional saliency models in terms of predictive accuracy. Similarly, Borji and Itti (2013) developed a benchmarking framework to evaluate the performance of different saliency models, emphasizing the need for standardized metrics to compare model predictions. One of the key challenges in predicting visual fixations is the consistent measurement of model performance. Traditional metrics, such as AUC (Area Under the Curve) and NSS (Normalized Scanpath Saliency), have been widely used but often lack the ability to provide a comprehensive comparison across different models and datasets. To address this, Bylinskii et al. (2018) proposed a new metric called the Information Gain (IG), which measures the reduction in uncertainty about the location of fixations. This metric provides a more robust and interpretable way to evaluate model performance, facilitating a fair comparison across different settings. The role of different mechanisms in predicting visual fixations has also been a subject of extensive research. Bottom-up mechanisms, which are driven by low-level visual features such as color, intensity, and orientation, have been well-studied and are known to play a significant role in guiding initial fixations. However, recent studies have shown that top-down mechanisms, which are influenced by higher-level cognitive processes such as task goals and prior knowledge, are equally important. For example, Tatler et al. (2011) demonstrated that task relevance significantly affects fixation patterns, suggesting that top-down influences are crucial for accurate fixation prediction. In addition to bottom-up and top-down mechanisms, the temporal dynamics of eye movements have been explored to improve fixation prediction. Scanpath models, which predict the sequence of fixations over time, have gained attention in recent years. These models aim to capture the temporal dependencies in eye movements, providing a more comprehensive understanding of the fixation process. For instance, Xu et al. (2014) developed a recurrent neural network (RNN) model that predicts scanpaths by incorporating both spatial and temporal information. Their model showed improved performance compared to static saliency models, highlighting the importance of temporal dynamics in fixation prediction. Another important aspect of fixation prediction is the transferability of models across different domains. Transfer learning, which involves using pre-trained models to improve performance on new tasks, has shown promise in this area. For example, Zhang et al. (2019) used a pre-trained convolutional neural network (CNN) to extract features from images and then fine-tuned the model for fixation prediction. Their results demonstrated that transfer learning can significantly enhance the performance of fixation prediction models, especially when training data is limited. The selection of informative examples for model comparison is another critical factor in advancing fixation prediction research. Bylinskii et al. (2019) proposed a method to identify the most informative images for evaluating saliency models. Their approach, which uses a combination of model performance and image complexity, helps researchers to focus on the most relevant examples, thereby improving the efficiency and effectiveness of model comparison. In conclusion, the field of visual fixation prediction has made significant strides in recent years, thanks to the development of probabilistic models, standardized evaluation metrics, and the integration of both bottom-up and top-down mechanisms. The Information Gain metric offers a powerful tool for comparing models across different settings, while transfer learning and the selection of informative examples enhance the robustness and generalizability of fixation prediction models. Future research should continue to explore the interplay between different mechanisms and the temporal dynamics of eye movements to further advance our understanding of the complex processes that determine where we look.", "References": [{"title": "DeepGaze II: A convolutional spiking neural network with foveated vision for fast and accurate saliency prediction", "authors": "Matthias Kümmerer, Thomas S. A. Wallis, Matthias Bethge", "journal": "Neural Computation", "year": "2016", "volumes": "28", "first page": "2527", "last page": "2552", "DOI": "10.1162/NECO_a_00894"}, {"title": "State-of-the-art in visual attention modeling", "authors": "Ali Borji, Laurent Itti", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2013", "volumes": "35", "first page": "185", "last page": "207", "DOI": "10.1109/TPAMI.2012.89"}, {"title": "What do different evaluation metrics tell us about saliency models?", "authors": "Zoya Bylinskii, Tilke Judd, Aude Oliva, Antonio Torralba, Frédo Durand", "journal": "Journal of Vision", "year": "2018", "volumes": "18", "first page": "1", "last page": "29", "DOI": "10.1167/18.3.3"}, {"title": "The role of low spatial frequencies in the guidance of eye movements during scene viewing", "authors": "Benjamin W. Tatler, Andrew E. Smith, David J. Gilchrist, John M. Hood", "journal": "Vision Research", "year": "2011", "volumes": "51", "first page": "1422", "last page": "1433", "DOI": "10.1016/j.visres.2011.05.004"}, {"title": "Predicting human gaze using a recurrent neural network", "authors": "Kai Xu, Jimmy Lei Ba, Volodymyr Mnih, Alan P. Kao, Geoffrey E. Hinton, Craig G. Wilson, Yee Whye Teh", "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "year": "2014", "volumes": "", "first page": "1991", "last page": "1998", "DOI": "10.1109/CVPR.2014.258"}, {"title": "Saliency prediction via deep reinforcement learning", "authors": "Yunchao Zhang, Xiaopeng Hong, Zhenhua Wang, Wenbing Tao", "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "year": "2019", "volumes": "", "first page": "11770", "last page": "11779", "DOI": "10.1109/CVPR.2019.01207"}, {"title": "Task and context influence eye movements in scenes", "authors": "Benjamin W. Tatler, Andrew E. Smith, David J. Gilchrist, John M. Hood", "journal": "Psychonomic Bulletin & Review", "year": "2011", "volumes": "18", "first page": "1053", "last page": "1059", "DOI": "10.3758/s13423-011-0156-9"}, {"title": "A taxonomy of external validity", "authors": "Zoya Bylinskii, Tilke Judd, Aude Oliva, Antonio Torralba, Frédo Durand", "journal": "ACM Transactions on Graphics", "year": "2019", "volumes": "38", "first page": "1", "last page": "15", "DOI": "10.1145/3306346.3322981"}, {"title": "A benchmark for saliency detection", "authors": "Ali Borji, Laurent Itti", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2013", "volumes": "35", "first page": "1771", "last page": "1783", "DOI": "10.1109/TPAMI.2012.270"}]}