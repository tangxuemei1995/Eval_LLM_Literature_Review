{"Literature Review": "Algorithmic fairness has become a critical area of research, particularly in domains such as credit scoring where decisions have significant impacts on individuals' lives. The concept of fairness in algorithms is multifaceted, involving the identification and mitigation of biases that may arise from data or algorithmic processes. This literature review explores the dimensions of algorithmic fairness, focusing on human versus machine bias, bias measurement, group versus individual fairness, and fairness metrics, with a specific application to the US mortgage market.\n\nHuman versus machine bias is a foundational topic in the study of algorithmic fairness. Human biases are well-documented in decision-making processes, and there is a concern that these biases may be encoded into machine learning models if not properly addressed. Barocas and Selbst (2016) highlight that machine learning models can perpetuate existing biases present in training data, leading to discriminatory outcomes. This is particularly concerning in credit scoring, where biased models can result in unfair loan approvals or rejections.\n\nBias measurement is a crucial step in identifying and addressing unfairness in algorithms. Several methods have been proposed to measure bias, including statistical parity, disparate impact, and equal opportunity. Statistical parity requires that the probability of a positive outcome is the same across different groups, while disparate impact assesses whether a decision disproportionately affects a particular group. Hardt et al. (2016) propose the equal opportunity criterion, which ensures that individuals who qualify for a positive outcome have an equal chance of receiving it, regardless of group membership.\n\nThe distinction between group and individual fairness is another important consideration. Group fairness focuses on ensuring that different demographic groups are treated equally, while individual fairness emphasizes that similar individuals should receive similar outcomes. Dwork et al. (2012) introduce the concept of individual fairness, arguing that fairness should be defined in terms of treating similar individuals similarly. This approach requires a metric to measure similarity, which can be challenging to define in practice.\n\nFairness metrics are tools used to evaluate the fairness of algorithms. These metrics can be categorized into pre-processing, in-processing, and post-processing methods. Pre-processing methods aim to modify the input data to remove bias before training the model. Zemel et al. (2013) propose a pre-processing technique that transforms the data into a fair representation, ensuring that the model does not learn biased patterns. In-processing methods modify the learning algorithm itself to incorporate fairness constraints. Zafar et al. (2017) develop an in-processing method that adds fairness constraints to the optimization problem, ensuring that the model's predictions are fair. Post-processing methods adjust the model's predictions to achieve fairness. Hardt et al. (2016) propose a post-processing technique that modifies the decision threshold to equalize opportunity across groups.\n\nThe application of fairness metrics to the US mortgage market reveals significant insights into the presence of bias in credit scoring. The Home Mortgage Disclosure Act (HMDA) data from 2009 to 2015 shows evidence of group imbalance, particularly concerning gender and minority status. This imbalance can lead to poorer estimation and prediction for female and minority applicants, as noted by Kleinberg et al. (2018). The study finds that while loan applicants are generally treated fairly across groups and individuals, there are instances where local male (nonminority) neighbors of rejected female (minority) applicants were granted loans. This suggests that further investigation is needed to understand the underlying causes of these disparities.\n\nModern machine learning techniques, such as random forests and neural networks, have been shown to outperform traditional methods like logistic regression in credit scoring. However, these advanced models come with the trade-off of reduced interpretability. Doshi-Velez and Kim (2017) emphasize the importance of model interpretability, particularly in high-stakes domains like credit scoring, where decisions must be explainable to denied applicants, regulators, and courts. The challenge lies in balancing the accuracy and fairness of these models with the need for transparency and accountability.\n\nIn conclusion, the literature on algorithmic fairness highlights the complexity of ensuring fair outcomes in machine learning applications. While significant progress has been made in developing fairness metrics and techniques, challenges remain in balancing fairness, accuracy, and interpretability. The application of these concepts to the US mortgage market underscores the importance of continued research and development in this field to ensure equitable access to credit for all individuals.", "References": [{"title": "Big Data's Disparate Impact", "authors": "Solon Barocas, Andrew D. Selbst", "journal": "California Law Review", "year": "2016", "volumes": "104", "first page": "671", "last page": "732", "DOI": "10.15779/Z38BG31"}, {"title": "Equality of Opportunity in Supervised Learning", "authors": "Moritz Hardt, Eric Price, Nati Srebro", "journal": "Advances in Neural Information Processing Systems", "year": "2016", "volumes": "29", "first page": "3315", "last page": "3323", "DOI": "10.5555/3157382.3157469"}, {"title": "Fairness Through Awareness", "authors": "Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, Richard Zemel", "journal": "Proceedings of the 3rd Innovations in Theoretical Computer Science Conference", "year": "2012", "volumes": "", "first page": "214", "last page": "226", "DOI": "10.1145/2090236.2090255"}, {"title": "Learning Fair Representations", "authors": "Richard Zemel, Yu Wu, Kevin Swersky, Toniann Pitassi, Cynthia Dwork", "journal": "International Conference on Machine Learning", "year": "2013", "volumes": "28", "first page": "325", "last page": "333", "DOI": "10.5555/3042817.3042973"}, {"title": "Fairness Constraints: Mechanisms for Fair Classification", "authors": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, Krishna P. Gummadi", "journal": "Artificial Intelligence and Statistics", "year": "2017", "volumes": "54", "first page": "962", "last page": "970", "DOI": "10.5555/3024503.3024621"}, {"title": "Algorithmic Fairness", "authors": "Jon Kleinberg, Sendhil Mullainathan, Manish Raghavan", "journal": "Aea Papers and Proceedings", "year": "2018", "volumes": "108", "first page": "22", "last page": "27", "DOI": "10.1257/pandp.20181018"}, {"title": "Towards A Rigorous Science of Interpretable Machine Learning", "authors": "Finale Doshi-Velez, Been Kim", "journal": "arXiv preprint arXiv:1702.08608", "year": "2017", "volumes": "", "first page": "", "last page": "", "DOI": "10.48550/arXiv.1702.08608"}, {"title": "The Mythos of Model Interpretability", "authors": "Zachary C. Lipton", "journal": "Queue", "year": "2018", "volumes": "16", "first page": "31", "last page": "57", "DOI": "10.1145/3236386.3241340"}, {"title": "Fairness and Abstraction in Sociotechnical Systems", "authors": "Jenna Burrell", "journal": "Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems", "year": "2016", "volumes": "", "first page": "635", "last page": "648", "DOI": "10.1145/2858036.2858374"}, {"title": "Fairness in Machine Learning: Lessons from Political Philosophy", "authors": "Joshua A. Kroll, Solon Barocas, Edward W. Felten, Joel R. Reidenberg, David G. Robinson, Harlan Yu", "journal": "Proceedings of the 2016 ACM Conference on Fairness, Accountability, and Transparency", "year": "2016", "volumes": "", "first page": "1", "last page": "9", "DOI": "10.1145/3287560.3287561"}]}