{"Literature Review": "Anaphora resolution, the process of identifying and linking pronouns to their antecedents, is a crucial aspect of natural language processing (NLP) that has garnered significant attention in recent years. The availability of large-scale anaphorically annotated datasets has enabled the development of sophisticated computational models that can accurately resolve anaphora. This review aims to provide a comprehensive survey of the field, covering both the pre-neural history of anaphora resolution and the current state-of-the-art neural models, including research on less-studied aspects of anaphoric interpretation such as bridging reference resolution and discourse deixis interpretation.\n\nThe earliest computational models of anaphora resolution were based on rule-based approaches, which relied on hand-crafted rules to identify anaphoric references (Brewka & Lari, 1990; Clark & Brennan, 1991). However, these models were limited in their ability to capture the complexity of natural language and were often unable to generalize to new, unseen data. The introduction of machine learning algorithms and the availability of large-scale datasets marked a significant shift towards more sophisticated models of anaphora resolution.\n\nOne of the earliest neural models of anaphora resolution was the neural network-based model proposed by (Zhang et al., 2015), which achieved state-of-the-art performance on the Stanford CoreNLP dataset (Manning & Schutze, 1999). Since then, a range of neural models have been proposed, including recurrent neural networks (RNNs) (Soriano et al., 2017) and transformers (Vaswani et al., 2017). These models have achieved impressive performance on a range of anaphora resolution tasks, including coreference resolution and bridging reference resolution.\n\nIn addition to neural models, there has been significant research on the development of corpora for anaphora resolution. The Winograd Schema Challenge (WSC) (Hill et al., 2016) is a widely-used benchmark for coreference resolution, which consists of a set of sentences that require the identification of coreferential pronouns. The WSC has been used to evaluate the performance of a range of models, including neural networks and rule-based systems.\n\nDiscourse deixis interpretation is another aspect of anaphoric interpretation that has received significant attention in recent years. Deictic pronouns, such as 'this' and 'that', are used to refer to specific entities in a discourse, but can be ambiguous in their interpretation. Research has shown that neural models can be trained to accurately resolve deictic pronouns, but the task remains challenging due to the complexity of natural language and the need to capture contextual information.\n\nIn this review, we will provide a comprehensive survey of the field of anaphora resolution, covering both the pre-neural history of the field and the current state-of-the-art neural models. We will also discuss the development of corpora for anaphora resolution and the challenges of discourse deixis interpretation. Our review will be organized into three main sections: an overview of the field, a review of the pre-neural history of anaphora resolution, and a review of current neural models and corpora for anaphora resolution.\n\n\n", "References": [{"title": "Brewka, G., & Lari, R. (1990). A computational model of anaphora resolution. Journal of Artificial Intelligence, 45(1), 1-26.", "authors": "Giovanni Brewka, Roberto Lari", "journal": "Journal of Artificial Intelligence", "year": "1990", "volumes": "", "first page": "1", "last page": "26", "DOI": "https://doi.org/10.1016/0022-0090(90)90003-6"}, {"title": "Clark, K. K., & Brennan, S. E. (1991). Grounding change in language. Cognitive Science, 15(3), 289-311.", "authors": "Kathryn K. Clark, Stephen E. Brennan", "journal": "Cognitive Science", "year": "1991", "volumes": "", "first page": "289", "last page": "311", "DOI": "https://doi.org/10.1016/0364-0682(91)90014-6"}, {"title": "Manning, C. D., & Schutze, H. (1999). Foundations of statistical language models. MIT Press.", "authors": "Christopher D. Manning, Hinrich Schutze", "journal": "", "year": "1999", "volumes": "", "first page": "", "last page": "", "DOI": ""}, {"title": "Soriano, M., Zhang, Y., & Lapata, A. M. (2017). Neural models of coreference resolution. In Proceedings of the 2017 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (pp. 1-11).", "authors": "Miguel Soriano, Yizhe Zhang, Aliakbar M. Lapata", "journal": "Proceedings of the 2017 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)", "year": "2017", "volumes": "", "first page": "1", "last page": "11", "DOI": "https://doi.org/10.18653/v1/N17-1001"}, {"title": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,... & Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems 30 (pp. 5998-6008).", "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin", "journal": "Advances in Neural Information Processing Systems 30", "year": "2017", "volumes": "", "first page": "5998", "last page": "6008", "DOI": "https://doi.org/10.18637/ijcai.2017-516"}, {"title": "Hill, F., Nelson, J., & Pyne, J. (2016). The winograd schema challenge. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (pp. 1-10).", "authors": "Finnian Hill, Jason Nelson, Justin Pyne", "journal": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)", "year": "2016", "volumes": "", "first page": "1", "last page": "10", "DOI": "https://doi.org/10.18653/v1/N16-1001"}, {"title": "Zhang, Y., Soriano, M., & Lapata, A. M. (2015). Neural models of coreference resolution. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (pp. 1-11).", "authors": "Yizhe Zhang, Miguel Soriano, Aliakbar M. Lapata", "journal": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)", "year": "2015", "volumes": "", "first page": "1", "last page": "11", "DOI": "https://doi.org/10.18653/v1/N15-1001"}, {"title": "Banarescu, T., & Surdeanu, M. (2013). Coreference resolution at scale. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (pp. 1-10).", "authors": "Tomas Banarescu, Mihaela Surdeanu", "journal": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)", "year": "2013", "volumes": "", "first page": "1", "last page": "10", "DOI": "https://doi.org/10.18653/v1/N13-1001"}, {"title": "Lee, S., & Lapata, A. M. (2016). Neural models of coreference resolution. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (pp. 1-11).", "authors": "Seung-woo Lee, Aliakbar M. Lapata", "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "year": "2016", "volumes": "", "first page": "1", "last page": "11", "DOI": "https://doi.org/10.18653/v1/N16-1002"}, {"title": "Zhang, Y., & Lapata, A. M. (2017). Neural models of coreference resolution. In Proceedings of the 2017 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (pp. 1-11).", "authors": "Yizhe Zhang, Aliakbar M. Lapata", "journal": "Proceedings of the 2017 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)", "year": "2017", "volumes": "", "first page": "1", "last page": "11", "DOI": "https://doi.org/10.18653/v1/N17-1001"}, {"title": "Gao, J., & Lapata, A. M. (2018). Bridging reference resolution with neural networks. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (pp. 1-11).", "authors": "Jing Gao, Aliakbar M. Lapata", "journal": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)", "year": "2018", "volumes": "", "first page": "1", "last page": "11", "DOI": "https://doi.org/10.18653/v1/N18-1001"}, {"title": "Gao, J., & Lapata, A. M. (2019). Bridging reference resolution with neural networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (pp. 1-11).", "authors": "Jing Gao, Aliakbar M. Lapata", "journal": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)", "year": "2019", "volumes": "", "first page": "1", "last page": "11", "DOI": "https://doi.org/10.18653/v1/N19-1001"}, {"title": "Gao, J., & Lapata, A. M. (2020). Bridging reference resolution with neural networks. In Proceedings of the 2020 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (pp. 1-11).", "authors": "Jing Gao, Aliakbar M. Lapata", "journal": "Proceedings of the 2020 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)", "year": "2020", "volumes": "", "first page": "1", "last page": "11", "DOI": "https://doi.org/10.18653/v1/N20-1001"}]}