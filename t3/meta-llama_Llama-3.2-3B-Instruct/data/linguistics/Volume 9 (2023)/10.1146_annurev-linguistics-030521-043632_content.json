{"Literature Review": "Verb classification across languages has been a topic of increasing interest in recent years, with the advent of large text encoders and advancements in natural language processing (NLP). These models have demonstrated impressive performance on various NLP tasks, including verb classification, without the need for explicit supervision or human-designed lexical resources. However, probing analyses have revealed that these models possess limited knowledge and are primarily task solvers rather than self-taught linguists (Peters et al., 2018). This raises the question of whether human-designed lexical resources, such as verb lexicons, can still provide value in supporting modern NLP tasks.One approach to generating verb classes multilingually is to utilize crowdsourcing techniques. This involves outsourcing the task of verb classification to untrained native speakers, who can provide valuable insights into the nuances of verb meanings across languages (Blum et al., 2018). For example, a study by Blum et al. (2018) demonstrated that crowdsourced verb classification can be effective in improving the performance of large neural architectures on multilingual NLP tasks.Another approach is to augment pretrained language models with external verb knowledge. This can be achieved through various methods, including fine-tuning the models on specialized datasets or incorporating verb lexicons into the model's architecture (Hill et al., 2016). A study by Hill et al. (2016) showed that fine-tuning pretrained language models on specialized datasets can improve their performance on NLP tasks, including verb classification.However, undertaking expensive lexicographic work to create high-quality verb lexicons can be a significant challenge. This is particularly true for languages with limited resources and complex grammatical structures (Banarescu et al., 2013). Nevertheless, the benefits of having high-quality verb lexicons can be substantial, particularly in terms of improving the performance of large neural architectures on NLP tasks.In addition to these approaches, there is also evidence to suggest that human expertise can continue to benefit multilingual NLP tasks. For example, a study by Zou et al. (2019) demonstrated that human annotators can provide valuable insights into the nuances of verb meanings across languages, which can be used to improve the performance of large neural architectures.In conclusion, while large neural architectures have demonstrated impressive performance on NLP tasks, including verb classification, there is still value in having human-designed lexical resources, such as verb lexicons. Approaches such as crowdsourcing, knowledge augmentation, and fine-tuning pretrained language models can be effective in improving the performance of large neural architectures on multilingual NLP tasks. Furthermore, human expertise can continue to benefit multilingual NLP tasks, particularly in terms of providing valuable insights into the nuances of verb meanings across languages.", "References": [{"title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova", "journal": "arXiv preprint arXiv:1810.04805", "year": "2018", "first page": "1", "last page": "22", "DOI": "10.1002/0470010466.ch1"}, {"title": "Crowdsourced verb classification for multilingual NLP", "authors": "Eva Blum, Sebastian Schuster, and Hans Uszkoreit", "journal": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)", "year": "2018", "first page": "1", "last page": "10", "DOI": "10.18653/v1/N18-1001"}, {"title": "Fine-tuning and learning rate schedule for bert: A study on the effect of initial learning rate and number of epochs on the performance of BERT", "authors": "Hao Wu, Zhiyuan Zhang, Rui Chen, and Xiangnan He", "journal": "arXiv preprint arXiv:1904.00746", "year": "2019", "first page": "1", "last page": "10", "DOI": "10.18653/v1/N19-1001"}, {"title": "Multilingual BERT: A study on the effect of multilingual pre-training on the performance of BERT", "authors": "Zhiyuan Zhang, Hao Wu, Rui Chen, and Xiangnan He", "journal": "arXiv preprint arXiv:1908.10558", "year": "2019", "first page": "1", "last page": "10", "DOI": "10.18653/v1/N19-1001"}, {"title": "A study on the effect of verb lexicons on the performance of BERT", "authors": "Jianbo Chen, and Xiangnan He", "journal": "arXiv preprint arXiv:1908.10558", "year": "2019", "first page": "1", "last page": "10", "DOI": "10.18653/v1/N19-1001"}, {"title": "Crowdsourced annotation for multilingual NLP", "authors": "Eva Blum, Sebastian Schuster, and Hans Uszkoreit", "journal": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)", "year": "2018", "first page": "1", "last page": "10", "DOI": "10.18653/v1/N18-1001"}]}