{"grounding": "Neural models greatly outperform grammar-based models across many tasks in modern computational linguistics. This raises the question of whether linguistic principles, such as the Principle of Compositionality, still have value as modeling tools. We review the recent literature and find that while an overly strict interpretation of compositionality makes it hard to achieve broad coverage in semantic parsing tasks, compositionality is still necessary for a model to learn the correct linguistic generalizations from limited data. Reconciling both of these qualities requires the careful exploration of a novel design space; we also review some recent results that may help in this exploration.", "generated_text": "This study explores the concept of compositionality in computational linguistics, a fundamental principle in natural language processing that posits that meaning is derived from the combination of meaning-bearing components. We investigate the application of neurosymbolic models, a class of neural networks that incorporate symbolic reasoning, to semantic parsing tasks. Our results demonstrate that these models can effectively capture the compositional structure of language, outperforming traditional neural network architectures. The findings have significant implications for the development of more accurate and interpretable language models, and highlight the potential of neurosymbolic models for advancing the field of computational linguistics.", "label": 0}