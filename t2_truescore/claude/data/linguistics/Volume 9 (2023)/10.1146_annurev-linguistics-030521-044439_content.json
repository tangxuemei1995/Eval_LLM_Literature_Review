{"grounding": "Neural models greatly outperform grammar-based models across many tasks in modern computational linguistics. This raises the question of whether linguistic principles, such as the Principle of Compositionality, still have value as modeling tools. We review the recent literature and find that while an overly strict interpretation of compositionality makes it hard to achieve broad coverage in semantic parsing tasks, compositionality is still necessary for a model to learn the correct linguistic generalizations from limited data. Reconciling both of these qualities requires the careful exploration of a novel design space; we also review some recent results that may help in this exploration.", "generated_text": "This paper explores the concept of compositionality in computational linguistics, focusing on its application in neural networks and neurosymbolic models. We review recent advancements in semantic parsing and examine how compositional approaches enhance the interpretability and generalization capabilities of language models. Our analysis reveals that incorporating compositional principles into deep learning architectures can lead to more robust and efficient natural language processing systems. We discuss the challenges and opportunities in bridging the gap between symbolic and neural approaches, highlighting the potential of hybrid models. Finally, we propose future research directions to further leverage compositionality in computational linguistics, aiming to improve both performance and explainability in language understanding tasks.", "label": 1}