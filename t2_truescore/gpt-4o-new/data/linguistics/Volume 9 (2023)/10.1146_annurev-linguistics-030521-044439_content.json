{"grounding": "Neural models greatly outperform grammar-based models across many tasks in modern computational linguistics. This raises the question of whether linguistic principles, such as the Principle of Compositionality, still have value as modeling tools. We review the recent literature and find that while an overly strict interpretation of compositionality makes it hard to achieve broad coverage in semantic parsing tasks, compositionality is still necessary for a model to learn the correct linguistic generalizations from limited data. Reconciling both of these qualities requires the careful exploration of a novel design space; we also review some recent results that may help in this exploration.", "generated_text": "Compositionality, a fundamental principle in linguistics, posits that the meaning of complex expressions derives from their parts and the rules used to combine them. In computational linguistics, this principle is crucial for tasks like semantic parsing and language understanding. Recent advancements in neural networks and neurosymbolic models have enhanced the ability to capture compositional structures, offering improved performance in natural language processing applications. This paper reviews the integration of compositionality in computational models, highlighting the strengths and limitations of current approaches. We discuss the implications for future research, emphasizing the potential of hybrid models to bridge the gap between symbolic and connectionist paradigms.", "label": 1}