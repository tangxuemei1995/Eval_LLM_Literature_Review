{"grounding": "Neural models greatly outperform grammar-based models across many tasks in modern computational linguistics. This raises the question of whether linguistic principles, such as the Principle of Compositionality, still have value as modeling tools. We review the recent literature and find that while an overly strict interpretation of compositionality makes it hard to achieve broad coverage in semantic parsing tasks, compositionality is still necessary for a model to learn the correct linguistic generalizations from limited data. Reconciling both of these qualities requires the careful exploration of a novel design space; we also review some recent results that may help in this exploration.", "generated_text": "This paper explores the role of compositionality in computational linguistics, focusing on how neural networks and neurosymbolic models can be leveraged to enhance semantic parsing. Compositionality, the principle that the meaning of a complex expression is determined by the meanings of its constituents and the rules used to combine them, is crucial for building robust and interpretable natural language processing systems. We review recent advances in neurosymbolic models that integrate symbolic reasoning with neural network architectures, demonstrating their effectiveness in capturing compositional structures. Experiments on benchmark datasets show that these models outperform purely neural approaches in tasks requiring compositional generalization, such as semantic parsing and question answering.", "label": 1}