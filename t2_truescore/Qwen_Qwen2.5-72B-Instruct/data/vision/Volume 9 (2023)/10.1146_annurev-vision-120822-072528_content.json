{"grounding": "As we navigate and behave in the world, we are constantly deciding, a few times per second, where to look next. The outcomes of these decisions in response to visual input are comparatively easy to measure as trajectories of eye movements, offering insight into many unconscious and conscious visual and cognitive processes. In this article, we review recent advances in predicting where we look. We focus on evaluating and comparing models: How can we consistently measure how well models predict eye movements, and how can we judge the contribution of different mechanisms? Probabilistic models facilitate a unified approach to fixation prediction that allows us to use explainable information explained to compare different models across different settings, such as static and video saliency, as well as scanpath prediction. We review how the large variety of saliency maps and scanpath models can be translated into this unifying framework, how much different factors contribute, and how we can select the most informative examples for model comparison. We conclude that the universal scale of information gain offers a powerful tool for the inspection of candidate mechanisms and experimental design that helps us understand the continual decision-making process that determines where we look.", "generated_text": "This study presents a comprehensive review and unifying framework for predicting visual fixations, integrating insights from eye movement research, saliency modeling, and information theory. We systematically compare various models, including those based on deep learning and transfer learning, to establish a benchmark for performance across different datasets. Our analysis reveals that while bottom-up saliency models remain effective, top-down models incorporating task-specific and contextual information significantly enhance predictive accuracy. We propose a taxonomy of fixation prediction models, categorizing them based on their underlying mechanisms and data requirements. The unifying framework we introduce not only facilitates model comparison but also highlights the importance of considering both low-level visual features and high-level cognitive processes. Our findings underscore the potential of transfer learning in improving model generalizability and suggest directions for future research, emphasizing the need for more diverse and ecologically valid datasets.", "label": 1}