{"Abstract": "This study presents a comprehensive review and unifying framework for predicting visual fixations, integrating insights from eye movement research, saliency modeling, and information theory. We systematically compare various models, including those based on deep learning and transfer learning, to establish a benchmark for performance across different datasets. Our analysis reveals that while bottom-up saliency models remain effective, top-down models incorporating task-specific and contextual information significantly enhance predictive accuracy. We propose a taxonomy of fixation prediction models, categorizing them based on their underlying mechanisms and data requirements. The unifying framework we introduce not only facilitates model comparison but also highlights the importance of considering both low-level visual features and high-level cognitive processes. Our findings underscore the potential of transfer learning in improving model generalizability and suggest directions for future research, emphasizing the need for more diverse and ecologically valid datasets."}