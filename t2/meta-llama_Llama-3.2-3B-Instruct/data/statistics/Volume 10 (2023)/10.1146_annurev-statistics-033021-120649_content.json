{"Abstract": "This study examines the fairness of risk algorithms in the context of criminal justice, leveraging techniques from optimal transport and conformation prediction to mitigate discrimination. By analyzing the performance of machine learning models in risk assessment, we identify biases in the data and algorithms that can lead to disparate treatment of marginalized groups. Our results show that traditional fairness metrics are insufficient to detect and address these issues, highlighting the need for more nuanced approaches. We propose a novel framework that incorporates conformation prediction to identify and correct biases in the data, and demonstrate its effectiveness in reducing discrimination. The findings of this research have significant implications for the development of fair risk algorithms in high-stakes applications, and contribute to the ongoing debate on the ethics of machine learning in the criminal justice system."}