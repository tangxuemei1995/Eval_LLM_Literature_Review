{"Abstract": "This study explores the concept of compositionality in computational linguistics, a fundamental principle in natural language processing that posits that meaning is derived from the combination of meaning-bearing components. We investigate the application of neurosymbolic models, a class of neural networks that incorporate symbolic reasoning, to semantic parsing tasks. Our results demonstrate that these models can effectively capture the compositional structure of language, outperforming traditional neural network architectures. The findings have significant implications for the development of more accurate and interpretable language models, and highlight the potential of neurosymbolic models for advancing the field of computational linguistics."}